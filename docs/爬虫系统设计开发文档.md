# 越南摩托汽车网站 - 爬虫系统设计开发文档

## 📋 文档概述

本文档详细记录越南摩托汽车网站数据采集爬虫系统的架构设计、实现方案、数据处理流程和运维管理。

### 系统信息
- **爬虫语言**: Python 3.x
- **数据导入**: TypeScript + Node.js
- **数据存储**: SQLite 3.x
- **Webhook集成**: n8n workflow
- **调度方式**: Cron / 手动触发

---

## 🎯 爬虫系统目标

### 1. 业务目标
- 自动采集越南市场摩托车和汽车的真实产品数据
- 获取最新的价格、配置、技术参数信息
- 为网站提供丰富的车型数据库
- 支持多品牌、多车型的数据更新

### 2. 技术目标
- 稳定可靠的数据采集
- 高质量的数据清洗
- 防止被反爬虫
- 可扩展的架构设计
- 自动化的数据更新流程

---

## 🏗️ 系统架构

### 1. 整体架构图

```
┌─────────────────────────────────────────────────────┐
│              数据源（Data Sources）                  │
│  • Honda Vietnam Official                          │
│  • Yamaha Vietnam Official                         │
│  • VinFast Website                                 │
│  • Dat Bike Website                                │
│  • 其他品牌官网/电商平台                            │
└─────────────────┬───────────────────────────────────┘
                  │ HTTP Requests
                  ↓
┌─────────────────────────────────────────────────────┐
│          Python 爬虫层（Crawler Layer）              │
│  ┌──────────────────────────────────────────────┐  │
│  │  Brand Crawlers（品牌爬虫）                   │  │
│  │  • HondaCrawler                              │  │
│  │  • YamahaCrawler                             │  │
│  │  • VinFastCrawler                            │  │
│  │  • DatBikeCrawler                            │  │
│  │  • SuzukiCrawler                             │  │
│  └──────────────────────────────────────────────┘  │
│  ┌──────────────────────────────────────────────┐  │
│  │  Data Processing（数据处理）                  │  │
│  │  • 数据清洗                                   │  │
│  │  • 格式标准化                                 │  │
│  │  • 字段验证                                   │  │
│  │  • 去重处理                                   │  │
│  └──────────────────────────────────────────────┘  │
│  ┌──────────────────────────────────────────────┐  │
│  │  Anti-Bot Measures（反爬虫对策）             │  │
│  │  • User-Agent 轮换                           │  │
│  │  • 随机延迟                                   │  │
│  │  • 代理支持                                   │  │
│  │  • 请求头伪装                                 │  │
│  └──────────────────────────────────────────────┘  │
└─────────────────┬───────────────────────────────────┘
                  │ JSON Output
                  ↓
┌─────────────────────────────────────────────────────┐
│        数据文件（Data Files）                        │
│  • motorcycles_data.json                           │
│  • motorcycles_enhanced_data.json                  │
│  • cars_data.json                                  │
└─────────────────┬───────────────────────────────────┘
                  │
                  ↓
┌─────────────────────────────────────────────────────┐
│     TypeScript 导入层（Import Layer）                │
│  ┌──────────────────────────────────────────────┐  │
│  │  Import Scripts（导入脚本）                   │  │
│  │  • import-motorcycles-enhanced.ts            │  │
│  │  • import-honda-complete.ts                  │  │
│  │  • import-yamaha-complete.ts                 │  │
│  │  • sync-motorcycles.ts                       │  │
│  └──────────────────────────────────────────────┘  │
│  ┌──────────────────────────────────────────────┐  │
│  │  Data Validation（数据验证）                  │  │
│  │  • 必填字段检查                               │  │
│  │  • 数据类型验证                               │  │
│  │  • 业务规则验证                               │  │
│  └──────────────────────────────────────────────┘  │
└─────────────────┬───────────────────────────────────┘
                  │ Sequelize ORM
                  ↓
┌─────────────────────────────────────────────────────┐
│         SQLite 数据库（Database）                    │
│  • motorcycles 表                                  │
│  • cars 表                                         │
│  • brands 表                                       │
│  • categories 表                                   │
└─────────────────────────────────────────────────────┘
```

---

### 2. 数据流向

```
数据源网站
    ↓
Python爬虫采集
    ↓
JSON文件输出
    ↓
TypeScript导入脚本
    ↓
Sequelize ORM
    ↓
SQLite数据库
    ↓
Express API
    ↓
前端展示
```

---

## 🐍 Python 爬虫实现

### 1. 爬虫文件结构

```
backend/src/scripts/crawlers/
├── motorcycle-crawler.py                # 基础版摩托车爬虫
├── motorcycle-crawler-enhanced.py       # 增强版摩托车爬虫
├── honda-complete-crawler.py           # Honda专用爬虫
├── yamaha-complete-crawler.py          # Yamaha专用爬虫
├── README.md                           # 爬虫使用说明
└── ENHANCED_README.md                  # 增强版说明
```

---

### 2. 基础爬虫类设计

**文件**: `motorcycle-crawler-enhanced.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
越南摩托车数据爬虫 - 增强版
功能：采集越南市场主流品牌的详细车型数据
"""

import json
import time
import random
from typing import List, Dict, Optional
import re

class MotorcycleEnhancedCrawler:
    """摩托车数据爬虫类"""
    
    def __init__(self):
        self.motorcycles = []
        self.output_file = 'motorcycles_enhanced_data.json'
        
    def random_delay(self, min_seconds=1, max_seconds=3):
        """随机延迟，避免过快请求"""
        time.sleep(random.uniform(min_seconds, max_seconds))
    
    def crawl_honda_enhanced(self) -> List[Dict]:
        """爬取Honda Vietnam详细数据"""
        # 实现代码
        pass
    
    def crawl_yamaha_enhanced(self) -> List[Dict]:
        """爬取Yamaha Vietnam详细数据"""
        # 实现代码
        pass
    
    def crawl_vinfast_electric(self) -> List[Dict]:
        """爬取VinFast电动摩托车数据"""
        # 实现代码
        pass
    
    def crawl_all(self):
        """爬取所有品牌数据"""
        all_motorcycles = []
        
        # 爬取各品牌
        all_motorcycles.extend(self.crawl_honda_enhanced())
        all_motorcycles.extend(self.crawl_yamaha_enhanced())
        all_motorcycles.extend(self.crawl_vinfast_electric())
        # ... 其他品牌
        
        self.motorcycles = all_motorcycles
        return all_motorcycles
    
    def save_to_json(self):
        """保存到JSON文件"""
        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump(self.motorcycles, f, ensure_ascii=False, indent=2)
        print(f'✅ 数据已保存到 {self.output_file}')
```

---

### 3. 数据字段规范

#### 摩托车数据字段（完整版）

```python
motorcycle_data = {
    # 基本信息
    'brand': 'Honda',                    # 品牌
    'model': 'Winner X',                 # 型号
    'year': 2024,                        # 年份
    'category': 'Xe thể thao',           # 类别
    'price_vnd': 48000000,               # 价格（越南盾）
    'fuel_type': 'Xăng',                 # 燃料类型
    
    # 动力系统（Engine）
    'engine_cc': 149,                    # 排量(cc)
    'engine_type': 'Xi-lanh đơn, 4 kỳ', # 发动机类型
    'power_hp': 17.1,                    # 功率(HP)
    'power_rpm': 9000,                   # 最大功率转速
    'torque_nm': 14.4,                   # 扭矩(Nm)
    'torque_rpm': 7000,                  # 最大扭矩转速
    'compression_ratio': '11.0:1',       # 压缩比
    'bore_stroke': '62.0 x 49.5 mm',     # 缸径x行程
    'valve_system': 'DOHC 4 van',        # 气门系统
    
    # 传动系统（Transmission）
    'transmission': 'Số sàn 6 cấp',      # 变速箱
    'clutch_type': 'Ly hợp ướt đa đĩa',  # 离合器类型
    'fuel_supply': 'Phun xăng điện tử',  # 供油方式
    'starter': 'Điện',                   # 启动方式
    'ignition': 'Full Transitor',        # 点火系统
    
    # 底盘系统（Chassis）
    'frame_type': 'Khung xương ống thép', # 车架类型
    'front_suspension': 'Giảm xóc ống lồng', # 前悬挂
    'rear_suspension': 'Phuộc đơn Pro-Link', # 后悬挂
    'front_brake': 'Đĩa đơn 276mm ABS',   # 前刹车
    'rear_brake': 'Đĩa đơn 220mm ABS',    # 后刹车
    'front_tire': '100/80-17M/C',         # 前轮胎
    'rear_tire': '130/70-17M/C',          # 后轮胎
    
    # 尺寸重量（Dimensions）
    'dimensions_mm': '2020 x 740 x 1100', # 长宽高
    'wheelbase_mm': 1328,                 # 轴距
    'ground_clearance_mm': 165,           # 离地间隙
    'seat_height_mm': 795,                # 座高
    'weight_kg': 127,                     # 重量
    'fuel_capacity_l': 4.7,               # 油箱容量
    
    # 配置特性（Features）
    'abs': True,                          # ABS防抱死
    'smart_key': False,                   # 智能钥匙
    'display_type': 'LCD toàn màu',       # 仪表类型
    'lighting': 'Đèn LED toàn bộ',        # 灯光系统
    'features': 'Phanh ABS 2 kênh, ...',  # 其他配置
    
    # 其他信息
    'description': '详细描述...',         # 车型描述
    'warranty': '3 năm hoặc 30,000 km',  # 保修
    'fuel_consumption': '1.8 L/100km',   # 油耗
    'colors': 'Đỏ-Đen-Trắng, Đen-Bạc',   # 颜色选项
}
```

---

#### 电动车额外字段

```python
electric_motorcycle = {
    # ... 基本字段相同 ...
    'fuel_type': 'Điện',                 # 燃料类型：电动
    
    # 电池系统
    'battery_type': 'Lithium-ion',       # 电池类型
    'battery_kwh': 2.5,                  # 电池容量(kWh)
    'battery_voltage': 60,               # 电压(V)
    'range_km': 80,                      # 续航里程(km)
    'charge_time_h': 3.5,                # 充电时间(小时)
    'charging_type': 'Sạc nhanh/chậm',   # 充电类型
    
    # 电机
    'motor_power_kw': 2.0,               # 电机功率(kW)
    'motor_torque_nm': 95,               # 电机扭矩(Nm)
    'max_speed_kmh': 50,                 # 最高速度(km/h)
}
```

---

### 3. 汽车数据字段

```python
car_data = {
    # 基本信息
    'brand': 'Toyota',
    'model': 'Camry',
    'variant': '2.5Q',                   # 配置版本
    'year': 2024,
    'category': 'Sedan',                 # 轿车/SUV/MPV
    'price_vnd': 1200000000,
    
    # 发动机
    'engine_capacity_l': 2.5,            # 排量(L)
    'engine_type': 'Inline 4',
    'power_hp': 181,
    'torque_nm': 231,
    'fuel_type': 'Xăng',
    
    # 传动
    'transmission': 'Tự động 8 cấp',     # 变速箱
    'drive_type': 'FWD',                 # 驱动方式
    
    # 车身
    'body_type': 'Sedan',
    'doors': 4,
    'seats': 5,
    'dimensions_mm': '4885 x 1840 x 1445',
    'wheelbase_mm': 2825,
    'trunk_capacity_l': 524,
    'curb_weight_kg': 1530,
    
    # 安全配置
    'airbags': 9,
    'abs': True,
    'esp': True,
    'traction_control': True,
    'parking_sensors': True,
    'camera': '360度',
    
    # 科技配置
    'infotainment_screen': '9 inch',
    'navigation': True,
    'bluetooth': True,
    'usb_ports': 2,
    'climate_control': 'Tự động 2 vùng',
    
    # 其他
    'description': '描述...',
    'colors': ['Trắng Ngọc Trai', 'Đen', 'Bạc'],
    'warranty': '3 năm không giới hạn km',
}
```

---

## 🔄 爬虫工作流程

### 1. 数据采集流程

```
开始
  ↓
选择目标品牌
  ↓
发送HTTP请求（带防爬虫措施）
  ↓
解析HTML/API响应
  ↓
提取数据字段
  ↓
数据清洗和标准化
  ↓
验证数据完整性
  ↓
保存到JSON文件
  ↓
结束
```

---

### 2. 单个品牌爬取示例

**Honda 爬虫实现**：

```python
def crawl_honda_enhanced(self) -> List[Dict]:
    """爬取Honda Vietnam详细数据"""
    print("🔍 开始爬取 Honda Vietnam 增强数据...")
    motorcycles = []
    
    # 模拟数据（实际应从官网爬取）
    honda_bikes = [
        {
            'brand': 'Honda',
            'model': 'Winner X',
            'year': 2024,
            'category': 'Xe thể thao',
            'price_vnd': 48000000,
            
            # 动力系统
            'engine_cc': 149,
            'engine_type': 'Xi-lanh đơn, 4 kỳ',
            'power_hp': 17.1,
            'power_rpm': 9000,
            'torque_nm': 14.4,
            'torque_rpm': 7000,
            
            # ... 更多字段
        },
        # ... 更多车型
    ]
    
    motorcycles.extend(honda_bikes)
    self.random_delay()  # 随机延迟
    
    print(f"✅ Honda: {len(motorcycles)} xe")
    return motorcycles
```

---

### 3. 主爬虫执行

```python
def main():
    """主函数"""
    crawler = MotorcycleEnhancedCrawler()
    
    print("=" * 60)
    print("🚀 开始爬取越南摩托车增强数据")
    print("=" * 60)
    print()
    
    # 爬取所有品牌
    motorcycles = crawler.crawl_all()
    
    print()
    print("=" * 60)
    print(f"✅ 爬取完成！总计: {len(motorcycles)} 辆摩托车")
    print("=" * 60)
    print()
    
    # 保存数据
    crawler.save_to_json()
    
    # 数据统计
    crawler.print_statistics()

if __name__ == '__main__':
    main()
```

---

## 📥 数据导入实现

### 1. TypeScript 导入脚本

**文件**: `src/scripts/import-motorcycles-enhanced.ts`

```typescript
import { dbConfig } from '../config/database';
import Motorcycle from '../models/Motorcycle';
import * as fs from 'fs';
import * as path from 'path';

async function importEnhancedMotorcycles() {
  try {
    console.log('🔄 开始导入增强版摩托车数据...\n');

    // 连接数据库
    await dbConfig.authenticate();
    console.log('✅ 数据库连接成功');

    // 同步数据库表
    await dbConfig.sync({ alter: true });
    console.log('✅ 数据库表结构已更新\n');

    // 读取JSON数据
    const jsonPath = path.join(__dirname, 'data', 'motorcycles_enhanced_data.json');
    
    if (!fs.existsSync(jsonPath)) {
      console.log('❌ 数据文件不存在:', jsonPath);
      process.exit(1);
    }

    const jsonData = fs.readFileSync(jsonPath, 'utf-8');
    const motorcyclesData = JSON.parse(jsonData);

    console.log(`📊 找到 ${motorcyclesData.length} 条摩托车数据\n`);

    // 批量导入
    let successCount = 0;
    let updateCount = 0;
    let errorCount = 0;

    for (const data of motorcyclesData) {
      try {
        // 检查是否已存在
        const existing = await Motorcycle.findOne({
          where: {
            brand: data.brand,
            model: data.model,
            year: data.year
          }
        });

        if (existing) {
          // 更新现有记录
          await existing.update({
            ...data,
            view_count: existing.view_count, // 保留浏览次数
          });
          console.log(`🔄 更新: ${data.brand} ${data.model} ${data.year}`);
          updateCount++;
        } else {
          // 创建新记录
          await Motorcycle.create({
            ...data,
            status: 'active',
            view_count: 0,
          });
          console.log(`✅ 添加: ${data.brand} ${data.model} ${data.year}`);
        }
        
        successCount++;
      } catch (error: any) {
        console.error(`❌ 失败: ${data.brand} ${data.model}`, error.message);
        errorCount++;
      }
    }

    // 显示统计
    console.log('\n' + '='.repeat(60));
    console.log('📊 导入统计');
    console.log('='.repeat(60));
    console.log(`✅ 成功: ${successCount} 条`);
    console.log(`🔄 更新: ${updateCount} 条`);
    console.log(`❌ 失败: ${errorCount} 条`);
    console.log('='.repeat(60));

  } catch (error) {
    console.error('❌ 导入过程出错:', error);
    process.exit(1);
  } finally {
    await dbConfig.close();
  }
}

importEnhancedMotorcycles();
```

---

### 2. 执行命令

```bash
# 1. 运行Python爬虫
cd /root/越南摩托汽车网站/backend/src/scripts/crawlers
python3 motorcycle-crawler-enhanced.py

# 2. 编译TypeScript
cd /root/越南摩托汽车网站/backend
npm run build

# 3. 导入数据到数据库
node dist/scripts/import-motorcycles-enhanced.js

# 可选：清空现有数据重新导入
node dist/scripts/import-motorcycles-enhanced.js --clear
```

---

## 🔌 Webhook 集成

### 1. n8n Workflow 集成

**使用场景**：
- 定时自动采集新闻
- 从外部RSS源同步内容
- 与第三方API集成

**架构**：
```
n8n Workflow
    ↓
HTTP Request (爬取数据)
    ↓
Data Processing (数据处理)
    ↓
POST to Webhook
    ↓
Backend API (/api/news/webhook)
    ↓
SQLite Database
```

---

### 2. Webhook 接口实现

**路由**: `POST /api/news/webhook`

**代码**: `src/routes/news.ts`

```typescript
// n8n Webhook - 创建新闻
router.post('/webhook', asyncHandler(NewsController.create));
```

**请求示例**：
```bash
curl -X POST http://localhost:4001/api/news/webhook \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Honda Winner X 2024 ra mắt",
    "content": "<p>Nội dung chi tiết...</p>",
    "summary": "Tóm tắt ngắn",
    "category": "Xe máy",
    "author_name": "Auto Crawler",
    "featured_image": "https://example.com/image.jpg",
    "status": "published"
  }'
```

---

### 3. n8n Workflow 示例

```json
{
  "nodes": [
    {
      "name": "Schedule Trigger",
      "type": "n8n-nodes-base.scheduleTrigger",
      "parameters": {
        "rule": {
          "interval": [{"field": "hours", "hoursInterval": 6}]
        }
      }
    },
    {
      "name": "HTTP Request",
      "type": "n8n-nodes-base.httpRequest",
      "parameters": {
        "url": "https://source-website.com/api/news",
        "method": "GET"
      }
    },
    {
      "name": "Process Data",
      "type": "n8n-nodes-base.code",
      "parameters": {
        "code": "// 数据清洗和转换\nreturn items.map(item => ({...}))"
      }
    },
    {
      "name": "Send to Backend",
      "type": "n8n-nodes-base.httpRequest",
      "parameters": {
        "url": "http://47.237.79.9:4001/api/news/webhook",
        "method": "POST",
        "bodyParameters": {
          "parameters": [...]
        }
      }
    }
  ]
}
```

---

## 🛡️ 反爬虫策略

### 1. User-Agent 轮换

```python
import random

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
]

def get_random_headers():
    return {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml',
        'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
```

---

### 2. 随机延迟

```python
def random_delay(self, min_seconds=1, max_seconds=3):
    """随机延迟，避免过快请求"""
    delay = random.uniform(min_seconds, max_seconds)
    time.sleep(delay)
```

**延迟策略**：
- 品牌间延迟：2-5秒
- 车型间延迟：0.5-2秒
- 请求失败后延迟：5-10秒

---

### 3. 请求重试机制

```python
def fetch_with_retry(self, url, max_retries=3):
    """带重试的请求"""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=self.get_random_headers())
            if response.status_code == 200:
                return response
            else:
                print(f"⚠️  状态码 {response.status_code}，重试 {attempt + 1}/{max_retries}")
        except Exception as e:
            print(f"❌ 请求失败: {e}，重试 {attempt + 1}/{max_retries}")
        
        time.sleep(2 ** attempt)  # 指数退避
    
    return None
```

---

### 4. 代理支持（可选）

```python
PROXIES = {
    'http': 'http://proxy-server:port',
    'https': 'https://proxy-server:port',
}

response = requests.get(url, headers=headers, proxies=PROXIES)
```

---

## 🧹 数据清洗

### 1. 价格处理

```python
def clean_price(price_str: str) -> int:
    """清洗价格数据"""
    # 移除货币符号和分隔符
    # "48.000.000 ₫" -> 48000000
    cleaned = re.sub(r'[^\d]', '', price_str)
    return int(cleaned) if cleaned else 0
```

---

### 2. 文本清洗

```python
def clean_text(text: str) -> str:
    """清洗文本数据"""
    if not text:
        return ''
    
    # 移除多余空格
    text = re.sub(r'\s+', ' ', text)
    
    # 移除特殊字符
    text = text.strip()
    
    return text
```

---

### 3. 字段标准化

```python
def normalize_category(category: str) -> str:
    """标准化分类名称"""
    category_map = {
        'sport': 'Xe thể thao',
        'scooter': 'Xe tay ga',
        'underbone': 'Xe số',
        'touring': 'Xe touring',
        'cruiser': 'Xe cruiser',
        'electric': 'Xe điện',
    }
    
    category_lower = category.lower()
    return category_map.get(category_lower, category)
```

---

### 4. 数据验证

```python
def validate_motorcycle_data(data: Dict) -> bool:
    """验证摩托车数据完整性"""
    required_fields = ['brand', 'model', 'year', 'price_vnd']
    
    # 检查必填字段
    for field in required_fields:
        if field not in data or not data[field]:
            print(f"❌ 缺少必填字段: {field}")
            return False
    
    # 验证数据类型
    if not isinstance(data['year'], int) or data['year'] < 2000:
        print(f"❌ 年份无效: {data['year']}")
        return False
    
    if not isinstance(data['price_vnd'], (int, float)) or data['price_vnd'] <= 0:
        print(f"❌ 价格无效: {data['price_vnd']}")
        return False
    
    return True
```

---

## 🎯 已实现的爬虫

### 1. 摩托车爬虫

#### 文件列表
| 文件 | 用途 | 品牌覆盖 |
|------|------|----------|
| `motorcycle-crawler.py` | 基础版爬虫 | 9个品牌，22款车型 |
| `motorcycle-crawler-enhanced.py` | 增强版爬虫 | 更详细的数据维度 |
| `honda-complete-crawler.py` | Honda专用 | Honda全系车型 |
| `yamaha-complete-crawler.py` | Yamaha专用 | Yamaha全系车型 |

---

#### 支持的品牌（9个）

**传统燃油品牌**：
- ✅ Honda (6款)
- ✅ Yamaha (5款)
- ✅ Suzuki (2款)
- ✅ Piaggio (1款)
- ✅ SYM (1款)

**电动车品牌**：
- ✅ VinFast (3款)
- ✅ Dat Bike (2款)
- ✅ Yadea (1款)
- ✅ Selex (1款)

**总计**: 22款摩托车

---

### 2. 导入脚本

#### 文件列表
| 文件 | 用途 |
|------|------|
| `import-motorcycles.ts` | 基础版导入 |
| `import-motorcycles-enhanced.ts` | 增强版导入 |
| `import-honda-complete.ts` | Honda数据导入 |
| `import-yamaha-complete.ts` | Yamaha数据导入 |
| `sync-motorcycles.ts` | 数据同步脚本 |
| `sync-database.ts` | 数据库同步工具 |

---

## 🔄 数据更新流程

### 1. 完整更新流程

```bash
#!/bin/bash
# 完整的数据更新流程

echo "🚀 开始更新摩托车数据..."

# 1. 运行爬虫
cd /root/越南摩托汽车网站/backend/src/scripts/crawlers
python3 motorcycle-crawler-enhanced.py

# 2. 检查数据文件
if [ ! -f "../data/motorcycles_enhanced_data.json" ]; then
    echo "❌ 爬虫数据文件不存在"
    exit 1
fi

# 3. 编译TypeScript
cd /root/越南摩托汽车网站/backend
npm run build

# 4. 导入到开发数据库
node dist/scripts/import-motorcycles-enhanced.js

# 5. 复制到生产数据库
echo "📦 复制到生产环境..."
cp database/vietnam_moto_auto.sqlite /var/www/vietnam-moto-auto/backend/database/

# 6. 重启生产服务
echo "🔄 重启后端服务..."
sudo systemctl restart vietnam-moto-backend

echo "✅ 数据更新完成！"
```

---

### 2. 增量更新流程

```typescript
// sync-motorcycles.ts
async function syncMotorcycles() {
  // 只更新价格和可用性，不覆盖其他字段
  const updates = await fetchLatestPrices();
  
  for (const update of updates) {
    await Motorcycle.update(
      { 
        price_vnd: update.price,
        availability: update.availability 
      },
      { 
        where: { 
          brand: update.brand,
          model: update.model 
        }
      }
    );
  }
}
```

---

## 📊 数据统计和监控

### 1. 爬虫统计

```python
def print_statistics(self):
    """打印数据统计"""
    print("\n" + "=" * 60)
    print("📊 数据统计")
    print("=" * 60)
    
    # 按品牌统计
    brand_count = {}
    for moto in self.motorcycles:
        brand = moto['brand']
        brand_count[brand] = brand_count.get(brand, 0) + 1
    
    print("\n📈 品牌分布:")
    for brand, count in sorted(brand_count.items()):
        print(f"  {brand}: {count} xe")
    
    # 按类别统计
    category_count = {}
    for moto in self.motorcycles:
        category = moto.get('category', 'Unknown')
        category_count[category] = category_count.get(category, 0) + 1
    
    print("\n📊 分类分布:")
    for category, count in sorted(category_count.items()):
        print(f"  {category}: {count} xe")
    
    # 价格统计
    prices = [m['price_vnd'] for m in self.motorcycles if 'price_vnd' in m]
    if prices:
        print(f"\n💰 价格范围:")
        print(f"  最低: {min(prices):,} ₫")
        print(f"  最高: {max(prices):,} ₫")
        print(f"  平均: {sum(prices)//len(prices):,} ₫")
```

---

### 2. 数据质量检查

```typescript
async function checkDataQuality() {
  // 检查缺失字段
  const motorcycles = await Motorcycle.findAll();
  
  const issues = motorcycles.filter(moto => {
    return !moto.engine_cc || 
           !moto.power_hp || 
           !moto.price_vnd ||
           !moto.images || moto.images.length === 0;
  });
  
  console.log(`📊 数据质量报告:`);
  console.log(`  总车型数: ${motorcycles.length}`);
  console.log(`  有问题车型: ${issues.length}`);
  console.log(`  数据完整率: ${((1 - issues.length/motorcycles.length) * 100).toFixed(2)}%`);
}
```

---

## 🔍 高级爬虫技术

### 1. JavaScript 渲染网站爬取

使用 Puppeteer 或 Playwright:

```python
from playwright.sync_api import sync_playwright

def crawl_with_browser(url):
    """使用浏览器爬取动态网站"""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        
        # 设置User-Agent
        page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 ...'
        })
        
        # 访问页面
        page.goto(url)
        
        # 等待内容加载
        page.wait_for_selector('.product-list')
        
        # 提取数据
        data = page.evaluate('''() => {
            return Array.from(document.querySelectorAll('.product-item'))
                .map(item => ({
                    title: item.querySelector('.title').textContent,
                    price: item.querySelector('.price').textContent,
                }));
        }''')
        
        browser.close()
        return data
```

---

### 2. API接口爬取

```python
def crawl_api_data(self):
    """爬取API接口数据"""
    api_url = "https://api.example.com/motorcycles"
    
    headers = {
        'User-Agent': self.get_random_user_agent(),
        'Accept': 'application/json',
        'Referer': 'https://example.com',
    }
    
    response = requests.get(api_url, headers=headers)
    
    if response.status_code == 200:
        data = response.json()
        return self.process_api_data(data)
    
    return []
```

---

### 3. 图片下载

```python
import requests
from PIL import Image
from io import BytesIO

def download_image(self, url: str, save_path: str):
    """下载并保存图片"""
    try:
        response = requests.get(url, headers=self.get_random_headers())
        
        if response.status_code == 200:
            img = Image.open(BytesIO(response.content))
            
            # 压缩图片
            img.thumbnail((1200, 800), Image.LANCZOS)
            
            # 保存
            img.save(save_path, 'JPEG', quality=85, optimize=True)
            print(f"✅ 图片已保存: {save_path}")
            return True
    except Exception as e:
        print(f"❌ 图片下载失败: {e}")
        return False
```

---

## ⏰ 定时任务

### 1. Cron 调度

```bash
# 编辑 crontab
crontab -e

# 每天凌晨3点更新摩托车数据
0 3 * * * cd /root/越南摩托汽车网站/backend/src/scripts/crawlers && python3 motorcycle-crawler-enhanced.py >> /var/log/crawler.log 2>&1

# 每天凌晨4点导入数据
0 4 * * * cd /root/越南摩托汽车网站/backend && node dist/scripts/import-motorcycles-enhanced.js >> /var/log/import.log 2>&1

# 每周一凌晨5点全量更新（清空重建）
0 5 * * 1 cd /root/越南摩托汽车网站/backend && node dist/scripts/import-motorcycles-enhanced.js --clear >> /var/log/import-full.log 2>&1
```

---

### 2. 自动化脚本

**文件**: `auto-update-data.sh`

```bash
#!/bin/bash
# 自动更新数据脚本

set -e

LOG_FILE="/var/log/moto-auto-update.log"
DATE=$(date '+%Y-%m-%d %H:%M:%S')

echo "[$DATE] 开始数据更新..." >> $LOG_FILE

# 1. 运行爬虫
cd /root/越南摩托汽车网站/backend/src/scripts/crawlers
python3 motorcycle-crawler-enhanced.py >> $LOG_FILE 2>&1

# 2. 导入数据
cd /root/越南摩托汽车网站/backend
npm run build >> $LOG_FILE 2>&1
node dist/scripts/import-motorcycles-enhanced.js >> $LOG_FILE 2>&1

# 3. 更新生产环境
cp database/vietnam_moto_auto.sqlite /var/www/vietnam-moto-auto/backend/database/
sudo systemctl restart vietnam-moto-backend

echo "[$DATE] 数据更新完成！" >> $LOG_FILE
```

---

## 🔐 安全与合规

### 1. 法律合规

**必须遵守**：
- ✅ robots.txt 规则
- ✅ 网站服务条款
- ✅ 数据隐私法规
- ✅ 合理使用原则

**建议**：
- 爬取频率控制在合理范围
- 仅爬取公开数据
- 标注数据来源
- 定期检查合规性

---

### 2. 请求频率限制

```python
class RateLimiter:
    """请求频率限制器"""
    
    def __init__(self, max_requests_per_minute=10):
        self.max_requests = max_requests_per_minute
        self.requests_time = []
    
    def wait_if_needed(self):
        """如果超过限制则等待"""
        now = time.time()
        
        # 清除1分钟前的记录
        self.requests_time = [t for t in self.requests_time if now - t < 60]
        
        # 如果达到限制，等待
        if len(self.requests_time) >= self.max_requests:
            wait_time = 60 - (now - self.requests_time[0])
            if wait_time > 0:
                print(f"⏳ 达到频率限制，等待 {wait_time:.1f} 秒...")
                time.sleep(wait_time)
                self.requests_time = []
        
        self.requests_time.append(now)
```

---

### 3. 错误处理和日志

```python
import logging

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawler.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

def crawl_with_error_handling(self):
    """带错误处理的爬取"""
    try:
        data = self.fetch_data()
        logger.info(f"成功爬取 {len(data)} 条数据")
        return data
    except requests.RequestException as e:
        logger.error(f"网络请求失败: {e}")
        return []
    except json.JSONDecodeError as e:
        logger.error(f"JSON解析失败: {e}")
        return []
    except Exception as e:
        logger.error(f"未知错误: {e}")
        return []
```

---

## 📦 数据存储和管理

### 1. JSON 文件格式

**输出文件**: `data/motorcycles_enhanced_data.json`

```json
[
  {
    "brand": "Honda",
    "model": "Winner X",
    "year": 2024,
    "category": "Xe thể thao",
    "price_vnd": 48000000,
    "fuel_type": "Xăng",
    "engine_cc": 149,
    "engine_type": "Xi-lanh đơn, 4 kỳ",
    "power_hp": 17.1,
    "power_rpm": 9000,
    "torque_nm": 14.4,
    "torque_rpm": 7000,
    "transmission": "Số sàn 6 cấp",
    "dimensions_mm": "2020 x 740 x 1100",
    "weight_kg": 127,
    "fuel_capacity_l": 4.7,
    "abs": true,
    "smart_key": false,
    "display_type": "LCD toàn màu",
    "description": "Động cơ xi-lanh đơn 149.2cc...",
    "features": "Phanh ABS 2 kênh, LCD màu, USB",
    "warranty": "3 năm hoặc 30,000 km"
  }
]
```

---

### 2. 数据文件位置

```
backend/src/scripts/
├── data/
│   ├── motorcycles_data.json              # 基础数据
│   ├── motorcycles_enhanced_data.json     # 增强数据
│   ├── honda_complete_data.json           # Honda完整数据
│   └── yamaha_complete_data.json          # Yamaha完整数据
```

---

### 3. 数据库导入策略

#### a) 清空重建（全量更新）
```bash
node dist/scripts/import-motorcycles-enhanced.js --clear
```

**场景**：
- 数据结构变更
- 修复数据错误
- 定期全量更新

---

#### b) 增量更新（保留现有）
```bash
node dist/scripts/import-motorcycles-enhanced.js
```

**逻辑**：
- 检查 brand + model + year 组合
- 存在则更新，不存在则新增
- 保留 view_count 等用户数据

---

## 🧪 测试与验证

### 1. 爬虫测试

```bash
# 测试单个品牌爬虫
python3 honda-complete-crawler.py

# 验证输出文件
cat ../data/motorcycles_enhanced_data.json | python3 -m json.tool | head -50

# 检查数据条数
cat ../data/motorcycles_enhanced_data.json | python3 -c "import json, sys; print(len(json.load(sys.stdin)))"
```

---

### 2. 导入测试

```bash
# 测试导入（不提交）
node dist/scripts/import-motorcycles-enhanced.js --dry-run

# 查看导入日志
tail -f /var/log/import.log
```

---

### 3. API验证

```bash
# 查询总数
curl "http://localhost:4001/api/vehicles/motorcycles?limit=100" | jq '.pagination.total'

# 按品牌查询
curl "http://localhost:4001/api/vehicles/motorcycles?brand=Honda" | jq '.data | length'

# 检查数据完整性
curl "http://localhost:4001/api/vehicles/motorcycles/1" | jq '.data | keys'
```

---

## 🚀 扩展开发

### 1. 添加新品牌

**步骤**：

1. 在爬虫中添加新方法：
```python
def crawl_kawasaki(self) -> List[Dict]:
    """爬取Kawasaki数据"""
    motorcycles = []
    
    kawasaki_bikes = [
        {
            'brand': 'Kawasaki',
            'model': 'Ninja 400',
            'year': 2024,
            # ... 其他字段
        }
    ]
    
    motorcycles.extend(kawasaki_bikes)
    return motorcycles
```

2. 在 `crawl_all()` 中调用：
```python
all_motorcycles.extend(self.crawl_kawasaki())
```

3. 重新运行爬虫和导入

---

### 2. 添加汽车爬虫

**文件**: `car-crawler.py`

```python
class CarCrawler:
    """汽车数据爬虫"""
    
    def crawl_toyota(self) -> List[Dict]:
        """爬取Toyota Vietnam数据"""
        cars = [
            {
                'brand': 'Toyota',
                'model': 'Camry',
                'variant': '2.5Q',
                'year': 2024,
                'category': 'Sedan',
                'price_vnd': 1200000000,
                'engine_capacity_l': 2.5,
                'power_hp': 181,
                'transmission': 'Tự động 8 cấp',
                'seats': 5,
                # ... 更多字段
            }
        ]
        return cars
```

---

### 3. 图片自动化处理

```python
def process_vehicle_images(self, vehicle_data):
    """处理车辆图片"""
    images_dir = f"../../../frontend/public/images/{vehicle_data['brand'].lower()}"
    os.makedirs(images_dir, exist_ok=True)
    
    downloaded_images = []
    
    for i, img_url in enumerate(vehicle_data.get('image_urls', [])):
        filename = f"{vehicle_data['model'].lower().replace(' ', '-')}-{i+1}.jpg"
        save_path = os.path.join(images_dir, filename)
        
        if self.download_image(img_url, save_path):
            downloaded_images.append(f"/images/{vehicle_data['brand'].lower()}/{filename}")
    
    vehicle_data['images'] = downloaded_images
    return vehicle_data
```

---

## 📈 监控与告警

### 1. 爬虫监控指标

```python
class CrawlerMonitor:
    """爬虫监控"""
    
    def __init__(self):
        self.metrics = {
            'start_time': None,
            'end_time': None,
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'data_count': 0,
            'errors': [],
        }
    
    def record_success(self):
        self.metrics['successful_requests'] += 1
    
    def record_failure(self, error):
        self.metrics['failed_requests'] += 1
        self.metrics['errors'].append(str(error))
    
    def generate_report(self):
        """生成监控报告"""
        duration = (self.metrics['end_time'] - self.metrics['start_time']).total_seconds()
        success_rate = (self.metrics['successful_requests'] / 
                       self.metrics['total_requests'] * 100 
                       if self.metrics['total_requests'] > 0 else 0)
        
        return {
            'duration_seconds': duration,
            'total_requests': self.metrics['total_requests'],
            'success_rate': f"{success_rate:.2f}%",
            'data_collected': self.metrics['data_count'],
            'errors': self.metrics['errors']
        }
```

---

### 2. 告警机制

```python
def send_alert(message):
    """发送告警通知"""
    # 方式1: 邮件通知
    # send_email(to='admin@example.com', subject='Crawler Alert', body=message)
    
    # 方式2: Webhook通知
    # requests.post('https://hooks.slack.com/...', json={'text': message})
    
    # 方式3: 日志记录
    logger.critical(f"🚨 ALERT: {message}")

# 使用示例
if success_rate < 50:
    send_alert(f"爬虫成功率过低: {success_rate}%")
```

---

## 🛠️ 开发工具

### 1. Python 依赖

**文件**: `requirements.txt`

```
requests>=2.31.0
beautifulsoup4>=4.12.0
lxml>=4.9.0
playwright>=1.40.0
Pillow>=10.0.0
python-dotenv>=1.0.0
```

安装：
```bash
pip3 install -r requirements.txt
```

---

### 2. 调试模式

```python
class MotorcycleCrawler:
    def __init__(self, debug=False):
        self.debug = debug
        
    def log(self, message):
        if self.debug:
            print(f"[DEBUG] {message}")
    
    def crawl_brand(self, brand):
        self.log(f"开始爬取 {brand}")
        # ... 爬取逻辑
        self.log(f"完成爬取 {brand}")

# 使用
crawler = MotorcycleCrawler(debug=True)
```

---

### 3. 数据对比工具

```bash
# 比较两次爬取的差异
diff <(jq -S . old_data.json) <(jq -S . new_data.json)

# 统计字段变化
python3 -c "
import json
old = json.load(open('old_data.json'))
new = json.load(open('new_data.json'))
print(f'旧数据: {len(old)} 条')
print(f'新数据: {len(new)} 条')
print(f'差异: {len(new) - len(old)} 条')
"
```

---

## 📊 数据质量保证

### 1. 数据验证规则

```typescript
interface DataValidationRules {
  // 必填字段
  required: ['brand', 'model', 'year', 'price_vnd'];
  
  // 数据类型
  types: {
    year: 'number',
    price_vnd: 'number',
    engine_cc: 'number',
    power_hp: 'number',
  };
  
  // 取值范围
  ranges: {
    year: { min: 2000, max: 2030 },
    price_vnd: { min: 1000000, max: 10000000000 },
    engine_cc: { min: 50, max: 2000 },
  };
}
```

---

### 2. 数据清洗流程

```typescript
function cleanMotorcycleData(raw: any): MotorcycleData {
  return {
    brand: cleanText(raw.brand),
    model: cleanText(raw.model),
    year: parseInt(raw.year),
    price_vnd: parsePrice(raw.price_vnd),
    engine_cc: parseFloat(raw.engine_cc) || null,
    power_hp: parseFloat(raw.power_hp) || null,
    // 处理数组字段
    images: Array.isArray(raw.images) ? raw.images : [],
    // 处理布尔字段
    abs: Boolean(raw.abs),
    smart_key: Boolean(raw.smart_key),
    // ... 其他字段
  };
}
```

---

## 🔍 故障排查

### 1. 常见问题

#### 问题1: 爬虫运行失败
```bash
# 检查Python版本
python3 --version

# 检查依赖
pip3 list | grep requests

# 查看错误日志
tail -f /var/log/crawler.log
```

---

#### 问题2: 数据导入失败
```bash
# 检查JSON格式
python3 -m json.tool data/motorcycles_enhanced_data.json

# 检查TypeScript编译
npm run build

# 查看详细错误
node dist/scripts/import-motorcycles-enhanced.js
```

---

#### 问题3: 数据不更新
```bash
# 检查数据库文件路径
ls -lh /var/www/vietnam-moto-auto/backend/database/vietnam_moto_auto.sqlite

# 检查文件修改时间
stat /var/www/vietnam-moto-auto/backend/database/vietnam_moto_auto.sqlite

# 重启后端服务
sudo systemctl restart vietnam-moto-backend
```

---

### 2. 调试技巧

```python
# 1. 打印请求详情
print(f"请求URL: {url}")
print(f"请求头: {headers}")
print(f"响应码: {response.status_code}")

# 2. 保存响应内容
with open('debug_response.html', 'w') as f:
    f.write(response.text)

# 3. 逐步验证
data = parse_data(response)
print(f"解析结果: {json.dumps(data, indent=2)}")
```

---

## 📚 最佳实践

### 1. 爬虫开发
- ✅ 遵守 robots.txt
- ✅ 设置合理的请求间隔
- ✅ 使用随机User-Agent
- ✅ 处理所有异常情况
- ✅ 记录详细日志

### 2. 数据管理
- ✅ 定期备份数据
- ✅ 版本控制数据文件
- ✅ 数据验证和清洗
- ✅ 监控数据质量

### 3. 生产部署
- ✅ 使用定时任务自动化
- ✅ 设置告警机制
- ✅ 监控爬虫健康状态
- ✅ 保留历史数据

---

## 🔄 完整使用示例

### 场景1: 首次采集数据

```bash
# 1. 运行爬虫
cd /root/越南摩托汽车网站/backend/src/scripts/crawlers
python3 motorcycle-crawler-enhanced.py

# 2. 验证数据
cat ../data/motorcycles_enhanced_data.json | python3 -m json.tool | head -50

# 3. 编译后端
cd /root/越南摩托汽车网站/backend
npm run build

# 4. 导入数据
node dist/scripts/import-motorcycles-enhanced.js

# 5. 查看结果
sqlite3 database/vietnam_moto_auto.sqlite "SELECT brand, model, price_vnd FROM motorcycles;"
```

---

### 场景2: 定期更新数据

```bash
# 1. 运行爬虫获取最新数据
python3 motorcycle-crawler-enhanced.py

# 2. 增量导入（保留现有数据）
node dist/scripts/import-motorcycles-enhanced.js

# 3. 同步到生产环境
sudo ./deploy.sh
```

---

### 场景3: 添加新品牌

```bash
# 1. 创建新品牌爬虫
cp honda-complete-crawler.py kawasaki-crawler.py

# 2. 修改爬虫代码
vim kawasaki-crawler.py

# 3. 测试运行
python3 kawasaki-crawler.py

# 4. 导入数据
node dist/scripts/import-motorcycles-enhanced.js
```

---

## 📞 相关资源

### 数据源参考
- Honda Vietnam: https://www.honda.com.vn
- Yamaha Vietnam: https://www.yamaha-motor.com.vn
- VinFast: https://vinfastauto.com
- Dat Bike: https://datbike.vn

### 技术文档
- Requests库: https://requests.readthedocs.io
- BeautifulSoup: https://www.crummy.com/software/BeautifulSoup/
- Playwright: https://playwright.dev/python/

---

## 🔗 相关文档

- [README.md](./README.md) - 项目总览
- [后端服务开发文档.md](./后端服务开发文档.md) - 后端API开发
- [数据库设计文档.md](./数据库设计文档.md) - 数据库设计
- [生产环境部署文档.md](./生产环境部署文档.md) - 部署说明

---

## 📝 待办事项

### 短期目标
- [ ] 添加更多摩托车品牌（Kawasaki、KTM、Ducati等）
- [ ] 实现汽车数据爬虫
- [ ] 添加图片自动下载功能
- [ ] 优化数据去重算法

### 中期目标
- [ ] 实现真实网站爬取（当前为模拟数据）
- [ ] 添加价格历史追踪
- [ ] 实现自动化测试
- [ ] 添加数据变化通知

### 长期目标
- [ ] 机器学习数据清洗
- [ ] 分布式爬虫架构
- [ ] 实时数据同步
- [ ] 多语言数据支持

---

**文档版本**: v1.0.0  
**最后更新**: 2025年10月12日  
**维护者**: 爬虫开发团队

---

## ⚠️ 重要声明

1. **合法合规**: 本爬虫系统仅用于学习和研究目的，采集的数据来源于公开渠道
2. **商业使用**: 如需商业使用，请确保遵守相关法律法规和网站服务条款
3. **数据准确性**: 爬取的数据仅供参考，实际以官方发布为准
4. **责任声明**: 使用本爬虫系统产生的任何问题，由使用者自行承担

